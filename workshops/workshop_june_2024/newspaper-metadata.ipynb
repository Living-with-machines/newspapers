{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b7e54c-286c-40e7-abf6-031ecbfe2137",
   "metadata": {},
   "source": [
    "# Newspaper metadata: exploration & sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40699773-4fad-4fc2-9ad0-e9025e372d7b",
   "metadata": {},
   "source": [
    "This notebook illustrates ways to access, filter, join and sample from a set of three CSV tables containing historical newspaper metadata released by the Living with Machines (LwM) project.\n",
    "\n",
    "Newspaper metadata refers to information *about* the titles, issues and articles that were published, such as their place and date of publication (as opposed to the newspaper content itself). By exploring this metadata, we can identify subsets of the newspaper collections of particular interest for a given research question. Given the large volume of textual data in the newspaper collections themselves, this process of sampling is an important step to focus subsequent analysis.\n",
    "\n",
    "In the final section of the notebook, a sample of metadata will be used to download the corresponding article text.\n",
    "\n",
    "**NOTE:** This is an interactive notebook. To work properly, the cells should be executed in order from top to bottom. Most of the cells will work automatically, but sometimes some manual intervention is needed (e.g. an exercise). These are highlighted with the the words \"**YOUR TURN**\".\n",
    "\n",
    "#### Contents:\n",
    "\n",
    "1. Newspaper metadata\n",
    "1. Data download\n",
    "1. Read CSV files\n",
    "1. Summary statistics\n",
    "1. Construct samples\n",
    "   - Filtered samples\n",
    "   - Impartial samples\n",
    "   - Prescriptive samples\n",
    "1. Full text access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_rows', 4)\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eff943",
   "metadata": {},
   "source": [
    "## 1. Newspaper metadata\n",
    "\n",
    "This notebook focusses primarily on data extracted from the LwM relational database of historical newspaper metadata ([lwmdb](https://github.com/Living-with-machines/lwmdb)).\n",
    "\n",
    "These extracts consist of three tables, containing one row per newspaper title, issue and item (respectively):\n",
    "\n",
    "| Table  | Rows        | Columns | Size   |\n",
    "|--------|-------------|---------|--------|\n",
    "| TITLES | 1,504       | 35      | 430 KB |\n",
    "| ISSUES | 2,425,752   | 6       | 147 MB |\n",
    "| ITEMS  | 223,012,941 | 7       | 15 GB |\n",
    "\n",
    "The term \"item\" refers to a semantic unit within a newspaper page (identified during the digitisation process). Typically these are newspaper articles but they may instead be adverts or illustrations. \n",
    "\n",
    "Due to their large size, the number of columns in the ISSUES and ITEMS tables is minimised. Additional metadata columns can be added by joining with the TITLES table (as demonstrated below).\n",
    "\n",
    "At 6.7 GB, the ITEMS table remains too large to work with conveniently on most personal computers. For this reason, we shall restrict our attention to a subset of newspaper items, which we refer to as \"open access\" because they are not subject to licensing rights which limit access to the newspaper content.\n",
    "\n",
    "| Table              | Rows      | Columns | Size   |\n",
    "|--------------------|-----------|---------|--------|\n",
    "| OPEN ACCESS ITEMS  | 9,326,170 | 7       | 616 MB |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6622c-35c1-497f-bdac-3a1146587c32",
   "metadata": {},
   "source": [
    "## 2. Data download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68e27d-a2db-46b0-a412-e092d682c8c5",
   "metadata": {},
   "source": [
    "This step only needs to be performed once, after which the downloaded metadata will be available in the `./data` subdirectory.\n",
    "\n",
    "The CSV files will be downloaded and stored locally in the following locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43d588-30f5-4b38-967c-4fd4728c2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_csv = \"./data/title_query_sorted.csv\"\n",
    "issues_csv = \"./data/issue_query_sorted.csv\"\n",
    "items_csv = \"./data/item_query_oa_sorted.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56eabe-48dc-4d5c-85b0-18bc0059653b",
   "metadata": {},
   "source": [
    "The largest CSV files are zipped in the remote storage account and will be stored as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251a585-ba7c-4578-a6d7-00d940458fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_zip = \"./data/issue_query_sorted.zip\"\n",
    "items_zip = \"./data/item_query_oa_sorted.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458ac3c",
   "metadata": {},
   "source": [
    "**NOTE:** Due to the large size of the ITEMS table we will work with a subset of the data, including only the \"open access\" material. \n",
    "\n",
    "<details>\n",
    "  <summary>Click here for instructions on how to obtain the complete ITEMS table.</summary>\n",
    "  \n",
    "To work with the complete ITEMS table, replace the file paths in the preceding two cells with the following values:\n",
    "```\n",
    "items_csv = \"./data/item_query_sorted.csv\"\n",
    "items_zip = \"./data/item_query_sorted.zip\"\n",
    "```\n",
    "Note, however, that the complete ITEMS CSV file is 15 GB. This is too large to fit into the working memory of most personal computers. Instead it can be read and processed in chunks using the `chunksize` parameter in [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f8956-140f-4421-9800-2f6c68b55fb2",
   "metadata": {},
   "source": [
    "#### Check for locally-stored data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c36be",
   "metadata": {},
   "source": [
    "If all of the data files are already available locally, you can skip forward to [Section 3](#readcsvfiles) to read the CSV files.\n",
    "\n",
    "Otherwise, execute the cells below to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f11070-aea5-4aaa-ad6b-51ff419a7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [titles_csv, issues_csv, items_csv, issues_zip, items_zip]\n",
    "if all(map(os.path.exists, all_files)):\n",
    "    print(\"All data files found. Skip forward to Section 3.\")\n",
    "else:\n",
    "    print(\"One or more data files were not found. Run the following cells to download data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd4585",
   "metadata": {},
   "source": [
    "#### Install AzCopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82770883-0d40-499c-95a8-bc7aff399146",
   "metadata": {},
   "source": [
    "To download files from Azure cloud storage we will need the AzCopy command line tool.\n",
    "\n",
    "The following cell checks whether AzCopy is already installed. If it is not, a suitable version can be found [here](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10#download-azcopy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36908783-1dff-4c1f-9d95-f1b5a7cc89cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all(map(os.path.exists, all_files)):\n",
    "    azcopy_version = !azcopy --version\n",
    "    if \"command not found\" in azcopy_version[0]:\n",
    "        print(\"Please install AzCopy using the link above.\")\n",
    "    else:\n",
    "        print(\"AzCopy is already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f643269-042c-4069-87e1-0908a744a898",
   "metadata": {},
   "source": [
    "#### Establish access to Azure blob storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8682b3",
   "metadata": {},
   "source": [
    "To download newspaper metadata files you will need a \"SAS token\" (shared access signature) granting read-only access to the project's Azure cloud storage.\n",
    "\n",
    "The SAS token is a string of characters (similar to a password) and will be provided by the workshop organisers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3dc2d1",
   "metadata": {},
   "source": [
    "**YOUR TURN:** To make the SAS token accessible, create a new file named `.env` in the same folder as this notebook. This file must contain the following line (with the provided SAS token between the quotes):\n",
    " ```\n",
    "SAS_TOKEN=\"COPY_SAS_TOKEN_HERE\"\n",
    " ```\n",
    "\n",
    "**NOTE:** After editing the `.env` file you will need to restart the Jupyter kernel and re-execute all of the code cells up to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2a57a",
   "metadata": {},
   "source": [
    "The next cell checks that a valid SAS token has been found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f576cca-1e4e-415f-9a99-110d1cbc7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_token = os.environ.get(\"SAS_TOKEN\")\n",
    "if not sas_token:\n",
    "    print(\"No SAS token found. Please follow the instructions above this cell.\")\n",
    "else:\n",
    "    matched = re.match(r\"^sv=.*se=(?P<expiry>20[0-9]{2}-[0-1][0-9]-[0-9]{2}).*sig=\", sas_token) \n",
    "    if not matched or not \"expiry\" in matched.groupdict():\n",
    "        print(\"Invalid SAS token.\")\n",
    "    else:\n",
    "        print(f\"SAS token found. Expiry date: {matched[\"expiry\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69511c54",
   "metadata": {},
   "source": [
    "#### Copy the data from Azure blob storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262e1a5",
   "metadata": {},
   "source": [
    "Download the TITLES table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad41522",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(titles_csv):\n",
    "    path = f\"'https://opennewspapers.blob.core.windows.net/lwmdb/title_query_sorted.csv?{sas_token}'\"\n",
    "    !azcopy copy {path} {titles_csv}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505dd7d",
   "metadata": {},
   "source": [
    "Download and unzip the ISSUES table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be770a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(issues_zip):\n",
    "    path = f\"'https://opennewspapers.blob.core.windows.net/lwmdb/issue_query_sorted.zip?{sas_token}'\"\n",
    "    !azcopy copy {path} {issues_zip}\n",
    "if not os.path.exists (issues_csv):\n",
    "    !unzip -j {issues_zip} -d \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81af5c",
   "metadata": {},
   "source": [
    "Download and unzip the ITEMS table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237301be-a38b-4e48-8d72-ad77f518217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(items_zip):\n",
    "    path = f\"'https://opennewspapers.blob.core.windows.net/lwmdb/{items_zip.split('/')[-1]}?{sas_token}'\"\n",
    "    !azcopy copy {path} {items_zip}\n",
    "if not os.path.exists (items_csv):\n",
    "    !unzip -j {items_zip} -d \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0341d8f-0b51-42dc-9ae3-3f13ba03dde9",
   "metadata": {},
   "source": [
    "#### Check the downloaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d924605-949b-4614-9f9c-998c4e3f1652",
   "metadata": {},
   "source": [
    "If the downloads were successful, the CSV files will now be available in the `data/` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac1007-2718-4d7d-a90d-5cce0f2db0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh ./data/*.csv\n",
    "if all(map(os.path.exists, (titles_csv, issues_csv, items_csv))):\n",
    "    print(\"All CSV files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5458cf",
   "metadata": {},
   "source": [
    "**YOUR TURN: (Optional)**\n",
    "\n",
    "The `data/` subdirectory should now contain three CSV files: `title_query_sorted.csv`, `issue_query_sorted.csv` and `item_query_sorted.csv`\n",
    "\n",
    "In the next section we will read these tables as a [`pandas`](https://pandas.pydata.org/) data frames. \n",
    "\n",
    "But you might also want to view them in a spreadsheet application, such as Microsoft Excel or OpenOffice Calc. In that case, you will need to use the \"Import\" function, to make sure the column separators are interpreted correctly.\n",
    "\n",
    "To view the table in Excel, go to the `File` menu and choose `Import`. Select the type `CSV file`, then navigate to the `data/` subdirectory and select the file named `title_query_sorted.csv`. \n",
    "\n",
    "On the next step, choose `Delimited` and on the following step select `Other` and enter the pipe character '|' in the box. Then click `Finish`.\n",
    "\n",
    "\n",
    "To view it in OpenOffice... TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4212b66-2074-41a6-ae94-ef7c9c3d8621",
   "metadata": {},
   "source": [
    "## 3. Read CSV files\n",
    "<a id='readcsvfiles'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa37ba-50ad-4ce3-82f6-e5e37511e80d",
   "metadata": {},
   "source": [
    "Now that we have a local copy of the CSV files, we are ready to read in the data.\n",
    "\n",
    "We begin by defining a function to read each of the CSV files with the correct column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bd2dd-44f3-4b75-9b53-1803d75ee0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file, int_cols=list(), float_cols=list(), date_cols=list(), **kwargs):\n",
    "    df = pd.read_csv(file, dtype=str, **kwargs)\n",
    "    # Remove whitespace from column names:\n",
    "    df.columns = df.columns.str.replace(' ', '')\n",
    "    for arg in int_cols:\n",
    "        # Replace whitespace strings with NaN and convert to Int64:\n",
    "        df[arg] = df[arg].str.replace(' ', '').replace('', np.nan).astype('Int64')\n",
    "    for arg in float_cols:\n",
    "        df[arg] = pd.to_numeric(df[arg], errors='coerce')\n",
    "    for arg in date_cols:\n",
    "        df[arg] = pd.to_datetime(df[arg])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d65083",
   "metadata": {},
   "source": [
    "#### Read the TITLES table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff440cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = read_csv(titles_csv, float_cols=['latitude', 'longitude'], delimiter='|')\n",
    "display(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b6304",
   "metadata": {},
   "source": [
    "#### Read the ISSUES table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = read_csv(issues_csv, date_cols=[\"date\"], delimiter='|')\n",
    "display(issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12264e-a798-4e66-b4c1-01959f61a8f7",
   "metadata": {},
   "source": [
    "#### Read the ITEMS table:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e79ad-6039-4490-ae81-ee90d05a2cf4",
   "metadata": {},
   "source": [
    "**Note:** the complete ITEMS table contains 223,012,941 rows and is 15 GB in size, too large for most personal computers to store in memory. For this reason, we restrict our attention to the \"open access\" material. This subset of the ITEMS table contains 9,326,170 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243cb7d-3c77-461a-89bf-4cd7ae1fee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = read_csv(items_csv, int_cols=['word_count'], float_cols=['ocr_quality_mean'], date_cols=[\"date\"], delimiter='|')\n",
    "display(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a1043b-23bd-44e4-ac96-9d30ce94d68d",
   "metadata": {},
   "source": [
    "## 4. Summary statistics\n",
    "\n",
    "To better understand the data, we'll calculate some summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db04e7",
   "metadata": {},
   "source": [
    "#### Data Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8f631",
   "metadata": {},
   "source": [
    "In the TITLES and ISSUES tables, the `data_provider` refers to the organisation that provided access to the digitised newspapers collections.\n",
    "\n",
    "There are four data providers:\n",
    " - **FindMyPast**: a private company which owns the rights to the [British Newspaper Archive](https://www.britishnewspaperarchive.co.uk/).\n",
    " - **JISC**: a not-for-profit organisation that funded two phases of newspaper digitisation between 2004 and 2009.\n",
    " - **Living with Machines**: a collaborative research project involving the British Library which ran from 2018 to 2023.\n",
    " - **Heritage Made Digital**: a British Library digitisation programme.\n",
    "\n",
    "This is an important variable in our data, because it determines the level of accessibility of the newspaper content. This content may include page images and/or text obtained from the images via Optical Character Recognition (OCR) software.\n",
    "\n",
    "Only material from the last two data providers, Living with Machines and Heritage Made Digital, is openly accessible without subscription or special agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2c541",
   "metadata": {},
   "source": [
    "So let's begin by counting the number of newspaper titles and issues, separately for each of the data providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b56ae-8668-46c6-8036-e5e54bfaf4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count = titles['data_provider'].value_counts().to_frame(name = \"Number of Titles\")\n",
    "issue_count = issues['data_provider'].value_counts().to_frame(name = \"Number of Issues\")\n",
    "data_providers = pd.concat([title_count, issue_count], axis = 1)\n",
    "data_providers.reset_index(inplace=True, names = \"Data Provider\")\n",
    "data_providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50561957",
   "metadata": {},
   "source": [
    "We see that the open access material includes 107,712 issues from 121 titles (although it is only ~5% of the whole)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c9daa",
   "metadata": {},
   "source": [
    "#### Temporal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f49c73",
   "metadata": {},
   "source": [
    "The ISSUES table includes the date of publication, ranging from 1720-05-02 to 1957-12-31.\n",
    "\n",
    "Here we plot a histogram showing the number of issues published per decade, from 1750 to 1950."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(1750, 1960, 10))\n",
    "plt.hist([date.year for date in issues['date']], bins=bins, rwidth=0.88, edgecolor='black')\n",
    "plt.title('Number of newspaper issues, by decade');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0cf6f",
   "metadata": {},
   "source": [
    "**YOUR TURN: (Optional)**\n",
    "\n",
    "Plot a histogram, similar to the one above, of the temporal distribution of (open access) newspaper **items**.\n",
    "\n",
    "<details>\n",
    "  <summary>Click here for a hint.</summary>\n",
    "  \n",
    "**Hint:**\n",
    "\n",
    "Copy the code in the preceding cell and paste it into a new cell below this one. \n",
    "\n",
    "Edit the code so that it refers to the table named `items`, instead of `issues`.\n",
    "\n",
    "Then change the histogram title inside the `plt.title` command.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f487499",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(1750, 1960, 10))\n",
    "plt.hist([date.year for date in items['date']], bins=bins, rwidth=0.88, edgecolor='black')\n",
    "plt.title('Number of OA newspaper items, by decade');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7dd5f",
   "metadata": {},
   "source": [
    "## 5. Construct samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8cf23f",
   "metadata": {},
   "source": [
    "This section contains some examples of how to create subsamples of the newspaper metadata. Three types of sample are considered:\n",
    "1. **Filtered samples**, obtained via simple filtering with respect to the values of one or more variables.\n",
    "2. **Impartial samples**, obtained by random selection which preserves multivariate distributions between variables.\n",
    "3. **Prescriptive samples**, obtained by random selection while prescribing a desired distribution with respect to particular variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08401db",
   "metadata": {},
   "source": [
    "### Filtered samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4735308",
   "metadata": {},
   "source": [
    "We begin with some simple examples of filtering the table of newspaper issues by time period and/or place of publication.\n",
    "\n",
    "First we filter on the `date` column to obtain all of the newspaper issues from the 1850s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_1850s = issues.query('18500101 <= date < 18591231').sort_values('date')\n",
    "display(issues_1850s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2918ad",
   "metadata": {},
   "source": [
    "The filtered tables take up much less space in memory, so we can join it to the TITLES table to add additional columns of metadata.\n",
    "\n",
    "To do this, we use the [`pd.merge`](https://pandas.pydata.org/docs/reference/api/pandas.merge.html) function, and join on the three columns that are common to both tables: the `publication_code`, `newspaper_id` and `data_provider`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_1850s = pd.merge(issues_1850s, titles, how='left', on=['publication_code', 'newspaper_id', 'data_provider'])\n",
    "display(issues_1850s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c3991",
   "metadata": {},
   "source": [
    "In the next example, we filter on the `historic_county_label` column to obtain all of the issues from the historic county of Dorset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_dorset = titles[titles['historic_county_label'] == 'Dorset']\n",
    "issues_dorset = issues[issues['publication_code'].isin(titles_dorset['publication_code'])]\n",
    "issues_dorset = pd.merge(issues_dorset, titles, how='left', on=['publication_code', 'newspaper_id', 'data_provider'])\n",
    "display(issues_dorset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ebf32",
   "metadata": {},
   "source": [
    "Combining the last two examples, we obtain all of the newspaper issues from the historic county of Dorset published during the 1850s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dorset_1850s = issues_dorset.query('18500101 <= date < 18591231').sort_values('date')\n",
    "display(issues_dorset_1850s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5939fc",
   "metadata": {},
   "source": [
    "The next example filters on the `data_provider` column to produce a table of open access newspaper issues. As explained above, these are the newspapers for which the data provider is either `Living with Machines` or `Heritage Made Digital`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All open access newspaper issues:\n",
    "oa_data_providers = ('Living with Machines', 'Heritage Made Digital')\n",
    "issues_oa = issues[issues['data_provider'].isin(oa_data_providers)]\n",
    "issues_oa = pd.merge(issues_oa, titles, how='left', on=['publication_code', 'newspaper_id', 'data_provider'])\n",
    "\n",
    "# Insert a new column for the year of publication:\n",
    "issues_oa.insert(3, \"year\", [d.year for d in issues_oa['date']])\n",
    "display(issues_oa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6949e30b",
   "metadata": {},
   "source": [
    "### Random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52dd0d",
   "metadata": {},
   "source": [
    "Next we'll create some random samples using the [`subsamplr`](https://github.com/Living-with-machines/subsamplr) tool, which makes it easy to construct samples that either reflect the underlying data, or prescribe the joint distributions of particular variables of interest.\n",
    "\n",
    "First we install `subsamplr` from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3191d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -q git+https://github.com/Living-with-machines/subsamplr@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bbfc3",
   "metadata": {},
   "source": [
    "Next we specify a set of variables to be used for sampling (referred to as \"dimensions\").\n",
    "\n",
    "As an example, we shall construct subsamples from the table of open access newspaper issues, taking into account the `year` and `political_leaning_label` variables.\n",
    "\n",
    "The `year` is a discrete variable. We must specify its type (integer), the discretisation size (one in this case), the variable range of interest (i.e. min & max) and the \"bin size\", which refers to the size of the bins into which all of the table rows will be assigned. The sampling procedure involves the random selection of a bin followed by the random selection of a row inside the bin. Here we specify a bin size of 10, meaning that each bin corresponds to a decade during the `year` range.\n",
    "\n",
    "The `political_leaning_label` is a categorical variable. We must specify its type (string) and the list of categories of interest.\n",
    "\n",
    "Any table rows whose values fall outside of the specified range (for a discrete variable) or list of categories (for a categorical variable) will be ignored when drawing the sample. For instance, here we have omitted the `independent` political leaning from the list of categories, so no articles from independent-leaning newspapers will be selected.\n",
    "\n",
    "**Note:** In addition to discrete and categorical variables, `subsamplr` also supports continuous numerical variables (for example, see the `ocr_quality_mean` variable defined [here](https://github.com/Living-with-machines/subsamplr/blob/main/examples/example_database_subsampling.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f08ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from io import StringIO\n",
    "config_str = \"\"\"\n",
    "variables:\n",
    "    - {name: 'year', class: 'discrete', type: 'int', min: 1850,\n",
    "        max: 1899, discretisation: 1, bin_size: 10}\n",
    "    - {name: 'political_leaning_label', class: 'categorical', type: 'str',\n",
    "        categories: ['liberal', 'neutral', 'conservative']}\n",
    "\"\"\"\n",
    "config = yaml.safe_load(StringIO(config_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ad057",
   "metadata": {},
   "source": [
    "Next we construct the collection of bins, given the above configuration, and assign the table rows (or sampling \"units\") to those bins.\n",
    "\n",
    "We identify each row (or \"unit\") by the `issue_id` variable, which is a unique identifier for the rows in the `issues_oa` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f54aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subsamplr import BinCollection, UnitGenerator\n",
    "bins = BinCollection.construct(config, track_exclusions=True)\n",
    "generator = UnitGenerator.generate_units(\n",
    "        issues_oa, unit_id=\"issue_id\", variables=bins.dimensions)\n",
    "units = list(generator)\n",
    "for unit, values in units:\n",
    "    bins.assign_to_bin(unit, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c25af",
   "metadata": {},
   "source": [
    "To summarise the binning process, we had 107,712 rows in our original table of open access newspaper issues. From those rows 47,355 sampling units were constructed. The other 60,357 had missing values in the `political_leaning_label` column.\n",
    "\n",
    "Based on the configuration above, 28,708 have been assigned to bins for sampling, and the other 18,647 were excluded because their values fall outside of the specified variable ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860098a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Open access issues: \\t{len(issues_oa)}\")\n",
    "print(f\"Sampling units: \\t{len(units)}\")\n",
    "print(f\"Binned: \\t\\t{bins.count_units()}\")\n",
    "print(f\"Excluded: \\t\\t{bins.count_exclusions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a7dd3",
   "metadata": {},
   "source": [
    "#### Impartial samples\n",
    "\n",
    "Before drawing a random sample, it is advisable to set a random seed. The seed is an arbitrary number used to initialise the random number generator in [`numpy`](https://numpy.org/), so that an identical sample can be generated in future by specifying the same seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76328183",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 654321\n",
    "from numpy.random import seed as npseed\n",
    "npseed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff9935",
   "metadata": {},
   "source": [
    "Drawing samples from our populated collection of bins is straightforward. Here we extract a sample of 2000 open access newpspaper issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc267cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2000\n",
    "sample_units = bins.select_units(k)\n",
    "issues_sample = issues_oa[issues_oa['issue_id'].isin(sample_units)]\n",
    "display(issues_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ced4d",
   "metadata": {},
   "source": [
    "The political leanings in the sample are similar to those in the full table of open access issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(issues_sample['political_leaning_label'].value_counts().reindex().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(issues_oa['political_leaning_label'].value_counts(dropna=False).reindex().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4782053",
   "metadata": {},
   "source": [
    "#### Prescriptive samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882cf783",
   "metadata": {},
   "source": [
    " Now we construct a sample in which the distributions of the configured variables are deliberately controlled.\n",
    "\n",
    "The specified range of values for the `year` variable (1850-1859) is split across five bins, one for each decade.\n",
    "\n",
    "The specified range of values for the `political_leaning_label` variable is split across three categories: 'liberal', 'neutral' and 'conservative'.\n",
    "\n",
    "We must prescribe (relative) weights for each variable and for each bin. Therefore we define our `weights` as a tuple of the same length as `bins.dimensions`, each element being itself a list of weights, one for each bin.\n",
    "\n",
    "Here we weight the last two decades twice as heavily as the first three decades, and we assign equal weight to all three political leaning categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_weights = [1, 1, 1, 2, 2]\n",
    "political_leaning_label_weights = [1, 1, 1]\n",
    "weights = (year_weights, political_leaning_label_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a93edec",
   "metadata": {},
   "source": [
    "Once again we set the random seed (for reproducibility), and then call `select_units()` to generate the sample, this time passing in the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091dda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "npseed(seed)\n",
    "sample_units = bins.select_units(k, weights=weights)\n",
    "issues_sample = issues_oa[issues_oa['issue_id'].isin(sample_units)]\n",
    "display(issues_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06adc77f",
   "metadata": {},
   "source": [
    "This time all three political leanings are represented approximately equally, because the sample was selected according to the prescribed uniform distribution on that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(issues_sample['political_leaning_label'].value_counts().reindex().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd73040",
   "metadata": {},
   "source": [
    "On the time axis, the weights were skewed towards to last two decades of the century."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e16cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(issues_sample['year'], bins=[1850, 1860, 1870, 1880, 1890, 1900], rwidth=0.88, edgecolor='black')\n",
    "plt.title('Issue counts in a prescriptive sample, by decade');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5cde7",
   "metadata": {},
   "source": [
    "## 6. Full text access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab3aeb",
   "metadata": {},
   "source": [
    "### Download full text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9c160",
   "metadata": {},
   "source": [
    "The following helper functions will be used to download zip archives containing the full text of all articles for a given newspaper title (identified by its `publication_code`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets a short acronym for the data provider for a given publication.\n",
    "def provider(publication_code):\n",
    "    data_provider = titles[titles.publication_code == publication_code].data_provider.item()\n",
    "    return ''.join([w[0] for w in data_provider.split()]).lower()\n",
    "\n",
    "# Gets the relative path to the article full text (zip file) for a given publication.\n",
    "def fulltext_zip(publication_code):\n",
    "    return f\"./data/{provider(publication_code)}-alto2txt/{publication_code}_plaintext.zip\"\n",
    "\n",
    "# Returns the could storage location of the article full text (zip file) for a given publication.\n",
    "def fulltext_blob(publication_code):\n",
    "    stub = 'https://opennewspapers.blob.core.windows.net/alto2txt/'\n",
    "    return f\"'{stub}{provider(publication_code)}-alto2txt/plaintext/{publication_code}_plaintext.zip?{sas_token}'\"\n",
    "\n",
    "# Downloads a zip file containing article full text for a given collection of publications.\n",
    "def download_fulltext(publication_codes):\n",
    "    for pub in publication_codes:\n",
    "        if os.path.exists(fulltext_zip(pub)):\n",
    "            continue\n",
    "        !azcopy copy {fulltext_blob(pub)} {fulltext_zip(pub)}\n",
    "\n",
    "download_fulltext(['0002083'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30fb49",
   "metadata": {},
   "source": [
    "### Extract article text\n",
    "\n",
    "TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def article_txt()\n",
    "\n",
    "def extract_fulltext(issues):\n",
    "    for issue in issues:\n",
    "        if os.path.exists()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
