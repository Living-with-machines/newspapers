{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-2ldxb0iUxG"
      },
      "source": [
        "# Epilogue\n",
        "## Using open source LLMs for analysing historical documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_e5BL9Ti3JG"
      },
      "source": [
        "Make sure you are using a [GPU](https://cloud.google.com/gpu) when running the code below.\n",
        "\n",
        "Go to **`Runtime`** and select **`Change runtime type`**, then select `T4 GPU` (or any other GPU available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxa9ES0zX9ic"
      },
      "outputs": [],
      "source": [
        "# install the transformer libraries\n",
        "!pip install -q -U \"transformers==4.40.0\" datasets --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IivyFPPNQ0t-"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') # disable warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27FpdOCLasrJ"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc9ROzqBjxan"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' # make sure you use a GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj-797e0Z9zr"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/kasparvonbeelen/lancaster-newspaper-workshop/wc/data/subsample500mixedocr-selected_mitch.csv')\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaf2ZEPWpkdu"
      },
      "outputs": [],
      "source": [
        "# for the purposes of this exercise, we remove both very short and long documents from the dataset\n",
        "df = df[df.word_count.between(10,250)].reset_index()\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ6LGyYkkYff"
      },
      "source": [
        "## The Hugging Face Hub\n",
        "\n",
        "In the example below, we will experiment with Llama-3-8B, a recent series of open-source LLMs created by Meta. To use Llama3 you need to:\n",
        "\n",
        "- Make an account on Hugging Face https://huggingface.co/\n",
        "- Go to the Llama-3-8B and sign the terms of use you should get a reply swiftly https://huggingface.co/meta-llama/Meta-Llama-3-8B\n",
        "- Create a user access token with read access: https://huggingface.co/docs/hub/en/security-tokens\n",
        "- Run the code cell below to log into the Hugging Face hub. Copy-paste the access token\n",
        "- Reply `n` to the question 'Add token as git credential? (Y/n)'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnTdsILcavow",
        "outputId": "ead91adf-7397-40b4-a3c2-ac82986ba3be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NNKw2Rknc2C"
      },
      "source": [
        "## Load the LLM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3dS2EoMTnny"
      },
      "outputs": [],
      "source": [
        "# define the model, we use the instruct variant\n",
        "checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# instantiate a text generation pipeline\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=checkpoint,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device=\"cuda\",\n",
        ")\n",
        "\n",
        "# some fluff to improve the generation\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY1ucJhFnguw"
      },
      "source": [
        "## Prompting\n",
        "\n",
        "System message: describe how you want to the LLM to work, the behaviour you want it to exhibit\n",
        "User message: Content you want to process (or the LLM to act on).\n",
        "\n",
        "```python\n",
        "messages [\n",
        "  {\n",
        "    \"role\" : \"system\",\n",
        "    \"content\": \"<system prompt here>\"\n",
        "  },\n",
        "  {\n",
        "    \"role\" : \"user\",\n",
        "    \"content\": \"<user prompt here>\"\n",
        "  }\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNYCb3IqoRZ7"
      },
      "source": [
        "Define a message by articulating a system and user prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO5PEGlsUukK"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"\n",
        "          You are an helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "          Read the newspaper articles attentively and extract the required information.\n",
        "          Each newspaper article will be enclosed with triple hash tags (i.e. ###).\n",
        "          Don't make thigs up! If the information is not in the article then just say 'Dunno'\"\"\"\n",
        "              },\n",
        "\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Provide a short description of principal characters portrayed newspaper article?\n",
        "                  ###{df.iloc[0].text}###\"\"\"\n",
        "              }\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MM2Wlv_Vw3y"
      },
      "outputs": [],
      "source": [
        "def get_completion(messages: list, temperature=.1, top_p=.1) -> str:\n",
        "  \"\"\"get completion for given system and user prompt\n",
        "    Arguments:\n",
        "    messages (list): a list containin a system and user message as\n",
        "      python dictionaries with keys 'role' and 'content'\n",
        "    temperature (float): regulate creativity of the text generation\n",
        "    top_p (float): cummulative probability included in the\n",
        "      generation process\n",
        "  \"\"\"\n",
        "  prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "      )\n",
        "\n",
        "  outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "      )\n",
        "  return outputs[0][\"generated_text\"][len(prompt):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxUuvcWLTjoZ"
      },
      "outputs": [],
      "source": [
        "print(get_completion(messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SML4OsbfXIk8"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "- Change the system message and ask the model to reply in medieval French.\n",
        "- Change the user message and ask the model to summarize the article and condense it to one sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CfZ-Q96omxn"
      },
      "outputs": [],
      "source": [
        "# Enter code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq0pWVJXoo10"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am1Ge38tXGP-"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"\"\"\n",
        "    You are an helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper articles attentively and extract the required information.\n",
        "    Each newspaper article will be enclosed with triple hash tags (i.e. ###).\n",
        "    Don't make thigs up! Answer in medieval French!\"\"\"},\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"Provide a short description of principal characters portrayed newspaper article?\n",
        "    ###{df.iloc[0].text}###\"\"\"}\n",
        "]\n",
        "\n",
        "print(get_completion(messages))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBMVp1OQXg3z"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"\"\"\n",
        "    You are an helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper articles attentively and extract the required information.\n",
        "    Each newspaper article will be enclosed with triple hash tags (i.e. ###).\n",
        "    Don't make thigs up! If the information is not in the article then just say 'Dunno'\"\"\"},\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"Summarize the article content in one sentence.\n",
        "    ###{df.iloc[0].text}###\"\"\"}\n",
        "]\n",
        "\n",
        "print(get_completion(messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNcYq_a-beX1"
      },
      "source": [
        "## Applying text generation to historical documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USeWpL8Ar-UX"
      },
      "source": [
        "### Example 1: Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw-N3XoUkRfu"
      },
      "outputs": [],
      "source": [
        "df_small = df.sample(5, random_state=1984).reset_index(drop=True)\n",
        "df_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJiND9t_dkE0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def apply_completions(item: pd.Series,\n",
        "                      system_message: str,\n",
        "                      user_message: str,\n",
        "                      text_column: str = 'text') -> str:\n",
        "  \"\"\"\n",
        "  Function that appl\n",
        "  Argument:\n",
        "    item (pd.Series): row from a pandas Dataframe\n",
        "    system_message (str): system prompt, specifies how the system\n",
        "      should behave in\n",
        "    user_message (str): user prompt, give instruction how to\n",
        "      process each historical. the documents itself will be append\n",
        "      from the 'text_column' argument\n",
        "    text_column (str): name of the text column\n",
        "  \"\"\"\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message}\n",
        "      ]\n",
        "  messages[1]['content'] += f\"\\n\\n###{item[text_column]}###\"\n",
        "  return  get_completion(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUEjoux9Y3ZV"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas() # use tqdm to view progress\n",
        "\n",
        "system_message = \"\"\"You are an helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper articles attentively and extract the required information.\n",
        "    Each newspaper article will be enclosed with triple hash tags (i.e. ###).\n",
        "    Don't make thigs up! If the information is not in the article then just say 'Dunno'\"\"\"\n",
        "user_message = \"Summarize the article content in one sentence.\"\n",
        "\n",
        "df_small['completion'] =  df_small.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n54N-ySxjuN2"
      },
      "outputs": [],
      "source": [
        "# get the summaries\n",
        "df_small['completion']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov1ySBNokZmj"
      },
      "source": [
        "### Example 2: Biography as microgenre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKWWeBVXoFaG"
      },
      "outputs": [],
      "source": [
        "df_small = df.sample(10, random_state=1984).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAYqIGPOheVr"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"You are an helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper articles attentively and extract structured information.\n",
        "    Each newspaper article will be enclosed with triple hash tags (i.e. ###).\n",
        "    Don't make thigs up!\"\"\"\n",
        "\n",
        "\n",
        "user_message = \"\"\"Who are the characters portrayed in the article?\n",
        "    Extract biographical from a newspaper article.\n",
        "    For each identified person return a nested Python dictionary with the key equal to the name of the individual.\n",
        "    The values conist of dictionaries that record specific attributes such as age, gender, nationality, profession ,place of birth etc.\n",
        "    The format has to be a Python dictionary, do not add extra text!\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDw5FzAta9Ku"
      },
      "outputs": [],
      "source": [
        "df_small['completion'] =  df_small.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJGmFs9Qq_Tu"
      },
      "outputs": [],
      "source": [
        "df_small['completion'][5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS6AD3QbpL0t"
      },
      "outputs": [],
      "source": [
        "eval(df_small['completion'][5].split('format:\\n\\n')[-1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE2fO-wRrL9f"
      },
      "outputs": [],
      "source": [
        "eval(df_small['completion'][4].split('format:\\n\\n')[-1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyZ-yv8brFdt"
      },
      "outputs": [],
      "source": [
        "eval(df_small['completion'][2].split('format:\\n\\n')[-1].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWAog9xdpG_q"
      },
      "source": [
        "### Example 3: OCR correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5N8e2qdpKBZ"
      },
      "outputs": [],
      "source": [
        "df_small_bad_ocr = df.sort_values('ocrquality')[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JXH_oypolYi"
      },
      "outputs": [],
      "source": [
        "user_message = \"\"\"Transcribe the text and correct typos and errors in the text caused by bad optical character recognition (OCR).\n",
        "Do not add any information that is not in the original text!\"\"\"\n",
        "\n",
        "df_small_bad_ocr['completion'] = df_small_bad_ocr.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjFbCSDYtNx-"
      },
      "outputs": [],
      "source": [
        "print(df_small_bad_ocr.iloc[0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLsWShc7tKGs"
      },
      "outputs": [],
      "source": [
        "print(df_small_bad_ocr.iloc[0]['completion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rc0U8WqsXiA"
      },
      "outputs": [],
      "source": [
        "print(df_small_bad_ocr.iloc[4]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5BxeDnttYZ7"
      },
      "outputs": [],
      "source": [
        "print(df_small_bad_ocr.iloc[4]['completion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpChnh_7q-gF"
      },
      "outputs": [],
      "source": [
        "df_small_bad_ocr.to_csv('newspaper_ocr_corrected.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-tHuljcsSoZ"
      },
      "source": [
        "## Combining document filtering and targeted prompting\n",
        "\n",
        "Below, we combine many the things we covered in the previous notebook. Instead of running an LLM on all the documents, we use regular expressions to select a relevant subset of newspaper articles and use the LLMs to extract structured information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg5Llg_7wOmQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "pattern = re.compile(r'\\baccident[s]{0,1}\\b',re.I) # compile a regex\n",
        "df_kw_sample = df[df.apply(lambda x: bool(pattern.findall(x.text)), axis=1)] # get only rows that match the regex\n",
        "\n",
        "# define the user message we retain the system message from previous examples\n",
        "user_message = \"\"\"Does the newspaper describe a historical accident? If not return an empty Python list'.\n",
        "If it does describe an accident extract, information on the people involved in the accident.\n",
        "Return a list of Python dictionaries. For each dictionary the key is equal to the name of the person.\n",
        "The values list charactertistics of this person such a gender, age and occupation.\n",
        "Only return the Python list and no additional text!\n",
        "\"\"\"\n",
        "\n",
        "# apply messages\n",
        "df_kw_sample['completion'] = df_kw_sample.progress_apply(apply_completions, user_message=user_message, system_message=system_message, axis=1)\n",
        "# save outputs\n",
        "df_kw_sample.to_csv('accidents.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsKxeC4RtrEp"
      },
      "outputs": [],
      "source": [
        "df_kw_sample['completion']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd_ZL0orx64i"
      },
      "outputs": [],
      "source": [
        "eval(df_kw_sample.iloc[0]['completion'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzavYR10sI_o"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Experiment with your own system and user message! Have fun :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyrtAOQ24cJO"
      },
      "outputs": [],
      "source": [
        "# enter code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzdNxsZPsKr5"
      },
      "source": [
        "# Fin."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Tq0pWVJXoo10"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
